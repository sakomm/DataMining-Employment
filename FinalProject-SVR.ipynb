{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis of Layoffs in the United States Tech Industry \n",
    "\n",
    "## Project Motivation and Background\n",
    "\n",
    "As Computer Science students about to enter the job market, we're concerned about the volatility of the tech industry. We want to analyze and create a system that can help people understand the markets, plan an exit strategy, and alleviate these concerns.\n",
    "\n",
    "## Project Goal:\n",
    "The goal of our project is to analyze trends in companies' recent layoffs in a variety of industries (aerospace, travel, retail, etc.) and detect patterns and trends.  This will be done by looking at the number of employees laid off, the location of the companies, their stages, and the funds they have raised.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['company', 'location', 'industry', 'total_laid_off',\n",
      "       'percentage_laid_off', 'date', 'stage', 'country', 'funds_raised'],\n",
      "      dtype='object')\n",
      "Unique values for 'company': 1438\n",
      "Total number of new columns: 256\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>total_laid_off</th>\n",
       "      <th>percentage_laid_off</th>\n",
       "      <th>date</th>\n",
       "      <th>funds_raised</th>\n",
       "      <th>industry_Aerospace</th>\n",
       "      <th>industry_Construction</th>\n",
       "      <th>industry_Consumer</th>\n",
       "      <th>industry_Crypto</th>\n",
       "      <th>industry_Data</th>\n",
       "      <th>...</th>\n",
       "      <th>stage_Series C</th>\n",
       "      <th>stage_Series D</th>\n",
       "      <th>stage_Series E</th>\n",
       "      <th>stage_Series F</th>\n",
       "      <th>stage_Series G</th>\n",
       "      <th>stage_Series H</th>\n",
       "      <th>stage_Series I</th>\n",
       "      <th>stage_Series J</th>\n",
       "      <th>stage_Subsidiary</th>\n",
       "      <th>stage_Unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N26</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vroom</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Greenhouse</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Megaport</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Airtasker</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      company  total_laid_off  percentage_laid_off        date  funds_raised  \\\n",
       "0         N26            71.0                 0.04  2023-04-28        1700.0   \n",
       "3       Vroom           120.0                 0.11  2023-04-27        1300.0   \n",
       "4  Greenhouse           100.0                 0.12  2023-04-27         110.0   \n",
       "7    Megaport            50.0                 0.16  2023-04-27          98.0   \n",
       "8   Airtasker            45.0                 0.20  2023-04-27          26.0   \n",
       "\n",
       "   industry_Aerospace  industry_Construction  industry_Consumer  \\\n",
       "0                   0                      0                  0   \n",
       "3                   0                      0                  0   \n",
       "4                   0                      0                  0   \n",
       "7                   0                      0                  0   \n",
       "8                   0                      0                  0   \n",
       "\n",
       "   industry_Crypto  industry_Data  ...  stage_Series C  stage_Series D  \\\n",
       "0                0              0  ...               0               0   \n",
       "3                0              0  ...               0               0   \n",
       "4                0              0  ...               0               0   \n",
       "7                0              0  ...               0               0   \n",
       "8                0              0  ...               1               0   \n",
       "\n",
       "   stage_Series E  stage_Series F  stage_Series G  stage_Series H  \\\n",
       "0               1               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "7               0               0               0               0   \n",
       "8               0               0               0               0   \n",
       "\n",
       "   stage_Series I  stage_Series J  stage_Subsidiary  stage_Unknown  \n",
       "0               0               0                 0              0  \n",
       "3               0               0                 0              0  \n",
       "4               0               0                 0              0  \n",
       "7               0               0                 0              0  \n",
       "8               0               0                 0              0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read in the data\n",
    "layoffs = pd.read_csv('layoffs.csv')\n",
    "layoffs.head()\n",
    "print(layoffs.columns)\n",
    "#we have to drop all rows with a blank percentage layed off cell\n",
    "layoffs.dropna(subset=['percentage_laid_off'], inplace=True)\n",
    "\n",
    "# one hot encoding for categorical variables \n",
    "print(f\"Unique values for 'company': {len(layoffs['company'].unique())}\")\n",
    "\n",
    "# Adding dummy variables for location, industry, stage, and country\n",
    "totalNewCols = len(layoffs['location'].unique()) + len(layoffs['industry'].unique()) + len(layoffs['stage'].unique()) + len(layoffs['country'].unique())\n",
    "print(f\"Total number of new columns: {totalNewCols}\")\n",
    "\n",
    "#loca = pd.get_dummies(layoffs['location'], prefix='location')\n",
    "indu = pd.get_dummies(layoffs['industry'], prefix='industry')\n",
    "stag = pd.get_dummies(layoffs['stage'], prefix='stage')\n",
    "#coun = pd.get_dummies(layoffs['country'], prefix='country')\n",
    "\n",
    "# drop the original columns\n",
    "layoffs.drop(['location', 'industry', 'stage', 'country'], axis=1, inplace=True)\n",
    "\n",
    "# concat the new columns\n",
    "#layoffs = pd.concat([layoffs, loca, indu, stag, coun], axis=1)\n",
    "layoffs = pd.concat([layoffs, indu, stag], axis=1)\n",
    "layoffs = layoffs[layoffs.stage_Unknown != 1]\n",
    "layoffs = layoffs[layoffs.industry_Other != 1]\n",
    "\n",
    "\n",
    "layoffs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_predictions(y_vec):\n",
    "    \"\"\"\n",
    "    Discretize a continuous layoff vector into distinct risk categories.\n",
    "    \"\"\"\n",
    "    # 0.00 - 0.17 = 0 # Lower Risk\n",
    "    # 0.17 - 0.25 = 1 # Medium Risk\n",
    "    # 0.25 - 0.37 = 2 # High Risk\n",
    "    # 0.37 - *    = 3 # Very High Risk\n",
    "    thresholds = [0.17, 0.25, 0.37]\n",
    "    shape = y_vec.shape[0], len(thresholds) + 1\n",
    "    rval = np.zeros(shape)\n",
    "    for i,t in enumerate(thresholds):\n",
    "        rval[:, i] = y_vec < t\n",
    "    rval[:, -1] = y_vec > t\n",
    "    return pd.DataFrame(rval, columns=['risk_low', 'risk_med', 'risk_high', 'risk_very_high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1074, 48)\n",
      "Testing set shape: (268, 48)\n"
     ]
    }
   ],
   "source": [
    "traing_data = layoffs.drop(['company', 'date'], axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_set = traing_data.sample(frac=0.8, random_state=0)\n",
    "test_set = traing_data.drop(train_set.index)\n",
    "\n",
    "print (f\"Training set shape: {train_set.shape}\")\n",
    "print (f\"Testing set shape: {test_set.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R ** 2 score: 0.2862418829588602\n",
      "variance: 0.3299445079835237\n",
      "mse: 0.04133577082898896\n",
      "mae: 0.12793103159905755\n",
      "\n",
      "Categorical accuracy: 0.503731343283582 (135/268)\n",
      "Micro-averaged F1 score: 0.8075657894736843\n",
      "Macro-averaged F1 score: 0.6874835056879944\n",
      "R^2: -0.1545937148797224\n",
      "Categorical predictions compated to actual: \n",
      "    risk  risk\n",
      "0      3     4\n",
      "1      1     3\n",
      "2      2     2\n",
      "3      3     2\n",
      "4      4     4\n",
      "5      1     4\n",
      "6      2     1\n",
      "7      2     2\n",
      "8      2     2\n",
      "9      2     4\n",
      "10     1     1\n",
      "11     1     1\n",
      "12     1     3\n",
      "13     2     2\n",
      "14     1     1\n",
      "15     1     1\n",
      "16     1     1\n",
      "17     4     1\n",
      "18     1     1\n",
      "19     2     1\n",
      "Raw predictions compared to actual:\n",
      "    percentage_laid_off  percentage_laid_off\n",
      "0              0.255912                 0.40\n",
      "1              0.157326                 0.25\n",
      "2              0.171619                 0.20\n",
      "3              0.319315                 0.17\n",
      "4              0.553271                 1.00\n",
      "5              0.134726                 0.40\n",
      "6              0.185372                 0.10\n",
      "7              0.171602                 0.20\n",
      "8              0.244623                 0.20\n",
      "9              0.195192                 0.45\n",
      "10             0.136474                 0.16\n",
      "11             0.092855                 0.02\n",
      "12             0.124888                 0.25\n",
      "13             0.170469                 0.20\n",
      "14             0.125004                 0.14\n",
      "15             0.107045                 0.08\n",
      "16             0.074574                 0.03\n",
      "17             0.372742                 0.11\n",
      "18             0.068796                 0.08\n",
      "19             0.184912                 0.14\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.config.set_visible_devices([], 'GPU') # GPU is broken rn oops\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import ridge_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy import arange\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "#### Preprocess\n",
    "# establish training set\n",
    "X_train = train_set.drop(['percentage_laid_off', 'total_laid_off'], axis=1)\n",
    "X_train = X_train.fillna(0)  # fill in blank cells\n",
    "\n",
    "#labels for our training set\n",
    "y_train = train_set[\"percentage_laid_off\"]\n",
    "\n",
    "# Scale/shift data:\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Perform PCA to reduce dimensionality:\n",
    "# This doesn't help; each new PCA dimension captures essentially the same\n",
    "# amount of variance.\n",
    "# pca = PCA(n_components=30, random_state=2684711)\n",
    "# pca.fit(X_train_scaled)\n",
    "# X_pca = pca.transform(X_train_scaled)\n",
    "\n",
    "# put values and labels into a csv file to look at\n",
    "X_train.to_csv('training_X.csv', index=False)\n",
    "y_train.to_csv('training_Y.csv', index=False)\n",
    "\n",
    "\n",
    "### Model Definition and Fitting\n",
    "## Ridge Regression model:\n",
    "# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# # define model\n",
    "# model = RidgeCV(alphas=arange(0.01, 1.0, 0.01), cv=cv)\n",
    "# # fit model (this line is what takes so long to run)\n",
    "# model.fit(X_train_scaled, y_train)\n",
    "\n",
    "## Neural network model:\n",
    "# model = keras.Sequential([\n",
    "#     layers.Input(shape=X_train_scaled.shape[1:]),\n",
    "#     layers.Dense(10, kernel_regularizer=regularizers.L1L2()),\n",
    "#     layers.Dense(10, kernel_regularizer=regularizers.L1L2()),\n",
    "#     layers.Dense(10, kernel_regularizer=regularizers.L1L2()),\n",
    "# #     layers.Dropout(.5),\n",
    "# #     layers.Dense(50),\n",
    "# #     layers.Dropout(.5),\n",
    "#     layers.Dense(1)\n",
    "# ])\n",
    "# model.summary()\n",
    "# model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "# model.fit(X_train_scaled, y_train, epochs=5, validation_split=0.1, batch_size=10)\n",
    "\n",
    "## SVR model:\n",
    "model = SVR(kernel='rbf', C=100, gamma=.00009, epsilon=0.005)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "### Validation and metrics collection\n",
    "#code to do validation with our test set\n",
    "X_test = test_set.drop(['percentage_laid_off', 'total_laid_off'], axis=1)\n",
    "X_test = X_test.fillna(0)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "y_test = test_set[\"percentage_laid_off\"]\n",
    "y_predict = model.predict(X_test_scaled)\n",
    "\n",
    "X_test.to_csv('test_X.csv',index=False)\n",
    "y_test.to_csv('test_Y.csv',index=False)\n",
    "\n",
    "y_predict_df = pd.DataFrame(y_predict, columns=['percentage_laid_off'])\n",
    "score = r2_score(y_test, y_predict)\n",
    "print(f'R ** 2 score: {score}')\n",
    "\n",
    "var = explained_variance_score(y_test, y_predict)\n",
    "print(f'variance: {var}')   \n",
    "\n",
    "mse = mean_squared_error(y_test, y_predict)\n",
    "print(f'mse: {mse}')\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_predict)\n",
    "print(f'mae: {mae}')\n",
    "\n",
    "y_test_cat = categorize_predictions(y_test)\n",
    "y_pred_cat = categorize_predictions(y_predict.flatten())\n",
    "\n",
    "print()\n",
    "res = np.all(y_test_cat == y_pred_cat, axis=1)\n",
    "print(f'Categorical accuracy: {res.sum()/res.shape[0]} ({res.sum()}/{res.shape[0]})')\n",
    "print(f\"Micro-averaged F1 score: {f1_score(y_test_cat, y_pred_cat, average='micro')}\")\n",
    "print(f\"Macro-averaged F1 score: {f1_score(y_test_cat, y_pred_cat, average='macro')}\")\n",
    "print(f\"R^2: {r2_score(y_test_cat, y_pred_cat)}\")\n",
    "\n",
    "def to_numeric(y_df):\n",
    "    return pd.DataFrame(np.argmax(y_df.to_numpy(), axis=1) + 1, columns=['risk'])\n",
    "\n",
    "print(f\"Categorical predictions compated to actual: \")\n",
    "print(pd.concat([to_numeric(y_pred_cat).head(20), to_numeric(y_test_cat.reset_index(drop=True)).head(20)], axis=1))\n",
    "print(\"Raw predictions compared to actual:\")\n",
    "print(pd.concat([y_predict_df.head(20), y_test.reset_index(drop=True).head(20)], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['funds_raised', 'industry_Aerospace', 'industry_Construction',\n",
      "       'industry_Consumer', 'industry_Crypto', 'industry_Data',\n",
      "       'industry_Education', 'industry_Energy', 'industry_Finance',\n",
      "       'industry_Fitness', 'industry_Food', 'industry_HR', 'industry_Hardware',\n",
      "       'industry_Healthcare', 'industry_Infrastructure', 'industry_Legal',\n",
      "       'industry_Logistics', 'industry_Manufacturing', 'industry_Marketing',\n",
      "       'industry_Media', 'industry_Other', 'industry_Product',\n",
      "       'industry_Real Estate', 'industry_Recruiting', 'industry_Retail',\n",
      "       'industry_Sales', 'industry_Security', 'industry_Support',\n",
      "       'industry_Transportation', 'industry_Travel', 'stage_Acquired',\n",
      "       'stage_Post-IPO', 'stage_Private Equity', 'stage_Seed',\n",
      "       'stage_Series A', 'stage_Series B', 'stage_Series C', 'stage_Series D',\n",
      "       'stage_Series E', 'stage_Series F', 'stage_Series G', 'stage_Series H',\n",
      "       'stage_Series I', 'stage_Series J', 'stage_Subsidiary',\n",
      "       'stage_Unknown'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Save trained model and scalers:\n",
    "import pickle\n",
    "with open(\"model.pkl\", 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "with open(\"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
